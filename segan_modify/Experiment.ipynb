{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.ops.variable_scope.VariableScope object at 0x7f6dadb51310>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sagemaker\n",
    "import tensorflow as tf\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "_role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=\"s3://sagemaker-us-east-1-002224604296/denoise/tfrecords/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pygmentize 'source/core/step_three_recognize_process/tools/train/\n",
    "\n",
    "#single gpu training\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# Canbe local_gpu, local, or ml.p3.2xlarge, ml.p3.8xlarge etc.\n",
    "instance_type='ml.p3.2xlarge'\n",
    "bucket= \"sagemaker-us-east-1-002224604296\"\n",
    "prefix=\"train-0611\"\n",
    "hyperparameters = {'servable_model_dir': '/opt/ml/model', 'epoch': 1, 'task_type': 'train', 'sagemaker': 'true'\n",
    "                  }\n",
    "# requirements.txt are not allowed in script mode (BYOS). https://stackoverflow.com/q/53530867/3252127\n",
    "# Hyperparameters are specified in the model_config.py file, we need to extract them out to use SageMaker automl features.\n",
    "segan_estimator = TensorFlow(entry_point='source/main_train_deploy.py',\n",
    "                             source_dir='.',\n",
    "                             model_dir='/opt/ml/model',\n",
    "                             role=_role,\n",
    "                             output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                             framework_version='1.15.2',\n",
    "                             hyperparameters=hyperparameters, #set weights_path if you are finetune a pretrained model\n",
    "                             py_version='py2',\n",
    "                             train_instance_type=instance_type,\n",
    "                             train_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = segan_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor=segan_estimator.deploy(initial_instance_count=1, instance_type='ml.g4dn.xlarge', endpoint_type='tensorflow-serving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def de_emph(y, coeff=0.95):\n",
    "    if coeff <= 0:\n",
    "        return y\n",
    "    x = np.zeros(y.shape[0], dtype=np.float32)\n",
    "    x[0] = y[0]\n",
    "    for n in range(1, y.shape[0], 1):\n",
    "        x[n] = coeff * x[n - 1] + y[n]\n",
    "    return x\n",
    "\n",
    "def pre_emph_np(x, coeff=0.95):\n",
    "    x0 = np.reshape(x[0], [1,])\n",
    "    diff = x[1:] - coeff * x[:-1]\n",
    "#     concat = tf.concat(0, [x0, diff])\n",
    "    concat = np.concatenate([x0, diff], 0)\n",
    "    return concat\n",
    "\n",
    "def pre_emph_test_np(coeff, wav):\n",
    "    x_preemph = pre_emph_np(wav, coeff)\n",
    "    return x_preemph\n",
    "    \n",
    "def clean_serving(x, canvas_size, preemph, predictor=None):\n",
    "    \"\"\" clean a utterance x\n",
    "        x: numpy array containing the normalized noisy waveform\n",
    "    \"\"\"\n",
    "    c_res = None\n",
    "    print('start timer')\n",
    "    Time1 = time.time()\n",
    "    for beg_i in range(0, x.shape[0], canvas_size):\n",
    "        if x.shape[0] - beg_i  < canvas_size:\n",
    "            length = x.shape[0] - beg_i\n",
    "            pad = canvas_size - length\n",
    "        else:\n",
    "            length = canvas_size\n",
    "            pad = 0\n",
    "        x_ = np.zeros((1, canvas_size))\n",
    "        print(x_[0].shape)\n",
    "        if pad > 0:\n",
    "            x_ = np.concatenate((x[beg_i:beg_i + length], np.zeros(pad)))\n",
    "        else:\n",
    "            x_ = x[beg_i:beg_i + length]\n",
    "        print('Cleaning chunk {} -> {}'.format(beg_i, beg_i + length))\n",
    "        canvas_w = None\n",
    "        # fdict = {self.gtruth_noisy[0]:x_}\n",
    "        # canvas_w = self.sess.run(self.Gs[0],\n",
    "        #                         feed_dict=fdict)[0]\n",
    "        test_example = {'wav_and_noisy': x_.tolist()}\n",
    "        if predictor == None:\n",
    "            canvas_w = x_\n",
    "        else:\n",
    "            canvas_w = predictor.predict(test_example)\n",
    "        canvas_w = np.reshape(np.array(canvas_w['predictions']),canvas_size)\n",
    "        print('canvas w shape: ', canvas_w.shape)\n",
    "        if pad > 0:\n",
    "            print('Removing padding of {} samples'.format(pad))\n",
    "            # get rid of last padded samples\n",
    "            canvas_w = canvas_w[:-pad]\n",
    "        if c_res is None:\n",
    "            c_res = canvas_w\n",
    "        else:\n",
    "            c_res = np.concatenate((c_res, canvas_w))\n",
    "    # deemphasize\n",
    "    c_res = de_emph(c_res, preemph)\n",
    "    print('finish {}'.format(time.time()-Time1))\n",
    "    return c_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_wav = \"../../record_noise/record_noise_28_clip_0.wav\"\n",
    "preemph = 0.95\n",
    "canvas_size = 2**14\n",
    "\n",
    "fm, wav_data = wavfile.read(test_wav)\n",
    "wavname = test_wav.split('/')[-1]\n",
    "wave = (2./65535.) * (wav_data.astype(np.float32) - 32767) + 1.\n",
    "\n",
    "x_pholder_np = pre_emph_test_np(preemph, wave)\n",
    "\n",
    "c_wave = clean_serving(x=x_pholder_np, canvas_size=canvas_size, preemph=preemph, predictor=predictor)\n",
    "\n",
    "wavfile.write(os.path.join('../../clean', wavname), 16e3, c_wave)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}